"""
Test complet du syst√®me ML PlannerIA
Teste tous les mod√®les, g√©n√©ration de donn√©es et int√©gration
"""

import sys
import os
import traceback
from pathlib import Path
import pandas as pd
import numpy as np

# Ajouter le chemin ML au PYTHONPATH
ml_path = Path(__file__).parent / "ml"
sys.path.insert(0, str(ml_path))

def test_imports():
    """Test des imports de tous les modules ML"""
    print("=" * 60)
    print("TEST 1: IMPORTS DES MODULES ML")
    print("=" * 60)
    
    modules_to_test = [
        'estimator_model',
        'risk_model', 
        'synthetic_generator',
        'train_estimator',
        'train_risk',
        'test_ml_system'
    ]
    
    results = {}
    
    for module_name in modules_to_test:
        try:
            module = __import__(module_name)
            print(f"‚úì {module_name}: OK")
            results[module_name] = "SUCCESS"
        except ImportError as e:
            print(f"‚úó {module_name}: ERREUR - {e}")
            results[module_name] = f"IMPORT_ERROR: {e}"
        except Exception as e:
            print(f"‚ö† {module_name}: ERREUR G√âN√âRALE - {e}")
            results[module_name] = f"ERROR: {e}"
    
    return results

def test_estimator_model():
    """Test du mod√®le d'estimation"""
    print("\n" + "=" * 60)
    print("TEST 2: MOD√àLE D'ESTIMATION")
    print("=" * 60)
    
    try:
        from estimator_model import EstimatorModel
        
        print("Initialisation du mod√®le d'estimation...")
        estimator = EstimatorModel()
        
        # Test avec donn√©es d'exemple
        test_data = {
            'complexity': 7,
            'team_size': 5,
            'experience_level': 3,
            'similar_projects': 2,
            'requirements_clarity': 8
        }
        
        print(f"Test avec donn√©es: {test_data}")
        
        # Test de pr√©diction
        prediction = estimator.predict(test_data)
        print(f"‚úì Pr√©diction: {prediction}")
        
        # Test de confiance si disponible
        if hasattr(estimator, 'predict_with_confidence'):
            prediction, confidence = estimator.predict_with_confidence(test_data)
            print(f"‚úì Pr√©diction avec confiance: {prediction} (confiance: {confidence})")
        
        return "SUCCESS"
        
    except Exception as e:
        print(f"‚úó Erreur dans le mod√®le d'estimation: {e}")
        traceback.print_exc()
        return f"ERROR: {e}"

def test_risk_model():
    """Test du mod√®le de risques"""
    print("\n" + "=" * 60)
    print("TEST 3: MOD√àLE DE RISQUES")
    print("=" * 60)
    
    try:
        from risk_model import RiskModel
        
        print("Initialisation du mod√®le de risques...")
        risk_model = RiskModel()
        
        # Test avec donn√©es de projet
        project_data = {
            'budget_variance': 0.15,
            'schedule_variance': -0.08,
            'team_turnover': 0.12,
            'complexity_score': 7.5,
            'stakeholder_involvement': 6
        }
        
        print(f"Test avec donn√©es projet: {project_data}")
        
        # Test d'√©valuation des risques
        risk_assessment = risk_model.assess_risk(project_data)
        print(f"‚úì √âvaluation des risques: {risk_assessment}")
        
        # Test des risques par cat√©gorie si disponible
        if hasattr(risk_model, 'categorize_risks'):
            risk_categories = risk_model.categorize_risks(project_data)
            print(f"‚úì Risques par cat√©gorie: {risk_categories}")
        
        return "SUCCESS"
        
    except Exception as e:
        print(f"‚úó Erreur dans le mod√®le de risques: {e}")
        traceback.print_exc()
        return f"ERROR: {e}"

def test_synthetic_generator():
    """Test du g√©n√©rateur de donn√©es synth√©tiques"""
    print("\n" + "=" * 60)
    print("TEST 4: G√âN√âRATEUR DE DONN√âES SYNTH√âTIQUES")
    print("=" * 60)
    
    try:
        from synthetic_generator import SyntheticGenerator
        
        print("Initialisation du g√©n√©rateur...")
        generator = SyntheticGenerator()
        
        # Test de g√©n√©ration de projets
        print("G√©n√©ration de projets synth√©tiques...")
        synthetic_projects = generator.generate_projects(n_projects=5)
        print(f"‚úì {len(synthetic_projects)} projets g√©n√©r√©s")
        
        # Afficher un exemple
        if synthetic_projects:
            print(f"‚úì Exemple de projet: {synthetic_projects[0]}")
        
        # Test de g√©n√©ration de t√¢ches si disponible
        if hasattr(generator, 'generate_tasks'):
            synthetic_tasks = generator.generate_tasks(n_tasks=10)
            print(f"‚úì {len(synthetic_tasks)} t√¢ches g√©n√©r√©es")
        
        return "SUCCESS"
        
    except Exception as e:
        print(f"‚úó Erreur dans le g√©n√©rateur: {e}")
        traceback.print_exc()
        return f"ERROR: {e}"

def test_model_files():
    """Test de la pr√©sence des fichiers de mod√®les entra√Æn√©s"""
    print("\n" + "=" * 60)
    print("TEST 5: FICHIERS DE MOD√àLES ENTRA√éN√âS")
    print("=" * 60)
    
    models_dir = Path("models")
    
    if not models_dir.exists():
        print("‚úó Dossier 'models' non trouv√©")
        return "NO_MODELS_DIR"
    
    model_files = list(models_dir.glob("*.pkl"))
    
    if not model_files:
        print("‚ö† Aucun fichier .pkl trouv√© dans le dossier models")
        return "NO_MODELS"
    
    print(f"‚úì {len(model_files)} fichiers de mod√®les trouv√©s:")
    for model_file in model_files:
        file_size = model_file.stat().st_size / 1024  # KB
        print(f"  - {model_file.name} ({file_size:.1f} KB)")
    
    # Test de chargement d'un mod√®le
    try:
        import pickle
        latest_model = model_files[0]
        with open(latest_model, 'rb') as f:
            model = pickle.load(f)
        print(f"‚úì Mod√®le {latest_model.name} charg√© avec succ√®s")
        return "SUCCESS"
    except Exception as e:
        print(f"‚úó Erreur de chargement du mod√®le: {e}")
        return f"LOAD_ERROR: {e}"

def test_training_scripts():
    """Test des scripts d'entra√Ænement"""
    print("\n" + "=" * 60)
    print("TEST 6: SCRIPTS D'ENTRA√éNEMENT")
    print("=" * 60)
    
    results = {}
    
    # Test train_estimator
    try:
        from train_estimator import main as train_estimator_main
        print("‚úì Script train_estimator import√©")
        results['train_estimator'] = "IMPORT_OK"
    except Exception as e:
        print(f"‚úó Erreur import train_estimator: {e}")
        results['train_estimator'] = f"IMPORT_ERROR: {e}"
    
    # Test train_risk
    try:
        from train_risk import main as train_risk_main
        print("‚úì Script train_risk import√©")
        results['train_risk'] = "IMPORT_OK"
    except Exception as e:
        print(f"‚úó Erreur import train_risk: {e}")
        results['train_risk'] = f"IMPORT_ERROR: {e}"
    
    return results

def test_data_flow():
    """Test du flux de donn√©es complet"""
    print("\n" + "=" * 60)
    print("TEST 7: FLUX DE DONN√âES COMPLET")
    print("=" * 60)
    
    try:
        # 1. G√©n√©rer des donn√©es synth√©tiques
        from synthetic_generator import SyntheticGenerator
        generator = SyntheticGenerator()
        project_data = generator.generate_projects(n_projects=1)[0]
        print("‚úì Donn√©es synth√©tiques g√©n√©r√©es")
        
        # 2. Tester l'estimation
        from estimator_model import EstimatorModel
        estimator = EstimatorModel()
        estimation = estimator.predict(project_data)
        print(f"‚úì Estimation calcul√©e: {estimation}")
        
        # 3. Tester l'√©valuation des risques
        from risk_model import RiskModel
        risk_model = RiskModel()
        risk_score = risk_model.assess_risk(project_data)
        print(f"‚úì Risque √©valu√©: {risk_score}")
        
        return "SUCCESS"
        
    except Exception as e:
        print(f"‚úó Erreur dans le flux de donn√©es: {e}")
        traceback.print_exc()
        return f"ERROR: {e}"

def test_performance():
    """Test de performance des mod√®les"""
    print("\n" + "=" * 60)
    print("TEST 8: PERFORMANCE DES MOD√àLES")
    print("=" * 60)
    
    try:
        import time
        
        # Test de performance de l'estimateur
        from estimator_model import EstimatorModel
        estimator = EstimatorModel()
        
        test_data = {'complexity': 5, 'team_size': 4}
        
        # Test de vitesse
        start_time = time.time()
        for _ in range(100):
            estimator.predict(test_data)
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 100 * 1000  # en ms
        print(f"‚úì Performance estimateur: {avg_time:.2f} ms par pr√©diction")
        
        if avg_time < 50:  # Moins de 50ms
            print("‚úì Performance excellente")
        elif avg_time < 200:
            print("‚ö† Performance correcte")
        else:
            print("‚úó Performance lente")
        
        return "SUCCESS"
        
    except Exception as e:
        print(f"‚úó Erreur test performance: {e}")
        return f"ERROR: {e}"

def run_all_tests():
    """Lance tous les tests"""
    print("D√âBUT DES TESTS ML PLANNERIA")
    print("=" * 80)
    
    test_results = {}
    
    # Lancer tous les tests
    test_results['imports'] = test_imports()
    test_results['estimator'] = test_estimator_model()
    test_results['risk_model'] = test_risk_model()
    test_results['synthetic'] = test_synthetic_generator()
    test_results['model_files'] = test_model_files()
    test_results['training_scripts'] = test_training_scripts()
    test_results['data_flow'] = test_data_flow()
    test_results['performance'] = test_performance()
    
    # R√©sum√© final
    print("\n" + "=" * 80)
    print("R√âSUM√â DES TESTS")
    print("=" * 80)
    
    success_count = 0
    total_tests = len(test_results)
    
    for test_name, result in test_results.items():
        if isinstance(result, dict):
            # Pour les tests avec sous-r√©sultats
            sub_success = sum(1 for v in result.values() if 'SUCCESS' in str(v))
            sub_total = len(result)
            print(f"{test_name:20}: {sub_success}/{sub_total} sous-tests r√©ussis")
            if sub_success == sub_total:
                success_count += 1
        else:
            status = "‚úì R√âUSSI" if "SUCCESS" in str(result) else "‚úó √âCHOU√â"
            print(f"{test_name:20}: {status}")
            if "SUCCESS" in str(result):
                success_count += 1
    
    print("\n" + "=" * 80)
    print(f"R√âSULTAT GLOBAL: {success_count}/{total_tests} tests r√©ussis")
    
    if success_count == total_tests:
        print("üéâ TOUS LES TESTS SONT PASS√âS! Le syst√®me ML est op√©rationnel.")
    elif success_count >= total_tests * 0.8:
        print("‚ö† La plupart des tests passent. Quelques ajustements n√©cessaires.")
    else:
        print("‚ùå Plusieurs tests √©chouent. V√©rification approfondie requise.")
    
    return test_results

if __name__ == "__main__":
    # Changer vers le r√©pertoire du projet
    try:
        os.chdir(Path(__file__).parent)
        print(f"R√©pertoire de travail: {os.getcwd()}")
    except Exception as e:
        print(f"Erreur changement r√©pertoire: {e}")
    
    # Lancer tous les tests
    results = run_all_tests()