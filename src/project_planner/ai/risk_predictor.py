"""
üîç Analyseur Pr√©dictif de Risques - PlannerIA
Machine Learning avanc√© pour d√©tection et pr√©diction de risques projets
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
import json
import pickle
from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, mean_squared_error
import warnings
warnings.filterwarnings('ignore')


class RiskCategory(Enum):
    TECHNICAL = "technical"
    FINANCIAL = "financial"
    OPERATIONAL = "operational"
    STRATEGIC = "strategic"
    LEGAL = "legal"
    MARKET = "market"
    TEAM = "team"


class RiskLevel(Enum):
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4


@dataclass
class RiskFactor:
    """Facteur de risque identifi√©"""
    factor_id: str
    name: str
    category: RiskCategory
    description: str
    impact_score: float  # 0-1
    probability: float   # 0-1
    detectability: float # 0-1 (1 = facile √† d√©tecter)
    risk_priority_number: float  # impact * probability * (1-detectability)
    

@dataclass
class PredictedRisk:
    """Risque pr√©dit par l'IA"""
    risk_id: str
    name: str
    category: RiskCategory
    predicted_level: RiskLevel
    probability: float
    potential_impact: float
    confidence: float
    triggers: List[str]
    early_warnings: List[str]
    mitigation_strategies: List[str]
    similar_cases: List[Dict[str, Any]]
    predicted_timeline: Optional[timedelta]
    prevention_actions: List[str]


@dataclass
class RiskProfile:
    """Profil de risque complet d'un projet"""
    project_id: str
    overall_risk_score: float
    risk_distribution: Dict[RiskCategory, float]
    predicted_risks: List[PredictedRisk]
    risk_trends: Dict[str, List[float]]
    critical_path_risks: List[str]
    mitigation_effectiveness: Dict[str, float]
    confidence_intervals: Dict[str, Tuple[float, float]]
    generated_at: datetime


class AIRiskPredictor:
    """Syst√®me IA de pr√©diction et analyse des risques"""
    
    def __init__(self):
        self.models = self._initialize_ml_models()
        self.risk_patterns = self._load_risk_patterns()
        self.historical_data = self._generate_synthetic_training_data()
        self.feature_encoders = {}
        self.scalers = {}
        self.trained = False
        
        # Entra√Ænement complet pour pr√©sentation
        self._train_initial_models()
    
    def _initialize_ml_models(self) -> Dict[str, Any]:
        """Initialise les mod√®les ML pour pr√©diction de risques"""
        
        return {
            # Classification des niveaux de risque
            'risk_classifier': RandomForestClassifier(
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42
            ),
            
            # Pr√©diction d'impact financier
            'impact_predictor': GradientBoostingRegressor(
                n_estimators=150,
                learning_rate=0.1,
                max_depth=8,
                random_state=42
            ),
            
            # Pr√©diction de probabilit√© d'occurrence
            'probability_predictor': GradientBoostingRegressor(
                n_estimators=100,
                learning_rate=0.15,
                max_depth=6,
                random_state=42
            ),
            
            # Clustering pour identification de patterns
            'pattern_detector': DBSCAN(
                eps=0.5,
                min_samples=3,
                metric='euclidean'
            ),
            
            # D√©tecteur d'anomalies
            'anomaly_detector': {
                'type': 'isolation_forest',
                'contamination': 0.1
            }
        }
    
    def _load_risk_patterns(self) -> Dict[str, Any]:
        """Charge les patterns de risques connus"""
        
        return {
            # Patterns bas√©s sur les caract√©ristiques du projet
            'project_patterns': {
                'large_team_communication': {
                    'trigger_conditions': {'team_size': ('>', 10)},
                    'risk_factors': ['Communication overhead', 'Coordination complexity'],
                    'historical_probability': 0.7,
                    'avg_impact': 0.6
                },
                'aggressive_timeline': {
                    'trigger_conditions': {'timeline_pressure': ('>', 0.8)},
                    'risk_factors': ['Quality compromise', 'Team burnout', 'Technical debt'],
                    'historical_probability': 0.8,
                    'avg_impact': 0.8
                },
                'new_technology_stack': {
                    'trigger_conditions': {'new_tech_ratio': ('>', 0.5)},
                    'risk_factors': ['Learning curve', 'Integration issues', 'Support limitations'],
                    'historical_probability': 0.6,
                    'avg_impact': 0.7
                },
                'distributed_team': {
                    'trigger_conditions': {'geographic_distribution': ('>', 3)},
                    'risk_factors': ['Time zone conflicts', 'Cultural differences', 'Communication delays'],
                    'historical_probability': 0.5,
                    'avg_impact': 0.5
                }
            },
            
            # Patterns temporels
            'temporal_patterns': {
                'end_of_sprint_rush': {
                    'timing': 'sprint_end',
                    'risk_increase': 0.3,
                    'affected_categories': [RiskCategory.TECHNICAL, RiskCategory.OPERATIONAL]
                },
                'holiday_periods': {
                    'timing': 'holidays',
                    'risk_increase': 0.4,
                    'affected_categories': [RiskCategory.TEAM, RiskCategory.OPERATIONAL]
                },
                'budget_review_periods': {
                    'timing': 'budget_review',
                    'risk_increase': 0.2,
                    'affected_categories': [RiskCategory.FINANCIAL, RiskCategory.STRATEGIC]
                }
            },
            
            # Corr√©lations entre risques
            'risk_correlations': {
                ('technical_debt', 'quality_issues'): 0.8,
                ('budget_overrun', 'timeline_delay'): 0.7,
                ('team_turnover', 'knowledge_loss'): 0.9,
                ('scope_creep', 'budget_overrun'): 0.6,
                ('poor_communication', 'requirement_changes'): 0.7
            },
            
            # Facteurs d'amplification
            'amplification_factors': {
                'domain_complexity': {
                    'fintech': 1.5,
                    'healthcare': 1.4,
                    'aerospace': 1.6,
                    'ecommerce': 1.1,
                    'web_app': 1.0
                },
                'team_experience': {
                    'junior': 1.4,
                    'mixed': 1.2,
                    'senior': 1.0,
                    'expert': 0.8
                }
            }
        }
    
    def _generate_synthetic_training_data(self) -> pd.DataFrame:
        """G√©n√®re des donn√©es d'entra√Ænement synth√©tiques bas√©es sur des projets r√©els"""
        
        np.random.seed(42)
        n_projects = 1000
        
        # Caract√©ristiques des projets
        data = []
        
        for i in range(n_projects):
            # Caract√©ristiques de base
            team_size = np.random.randint(3, 20)
            project_duration = np.random.randint(30, 365)
            budget = np.random.uniform(10000, 500000)
            complexity = np.random.choice(['low', 'medium', 'high', 'very_high'])
            domain = np.random.choice(['web_app', 'mobile', 'enterprise', 'fintech', 'ecommerce'])
            team_experience = np.random.choice(['junior', 'mixed', 'senior', 'expert'])
            
            # Variables d√©riv√©es
            team_size_factor = min(2.0, team_size / 10)
            timeline_pressure = np.random.uniform(0.2, 1.0)
            requirements_clarity = np.random.uniform(0.3, 1.0)
            stakeholder_engagement = np.random.uniform(0.2, 0.9)
            
            # G√©n√©ration des risques bas√©e sur les caract√©ristiques
            technical_risk = self._calculate_synthetic_risk(
                complexity, team_experience, timeline_pressure, risk_type='technical'
            )
            financial_risk = self._calculate_synthetic_risk(
                budget/100000, team_size_factor, timeline_pressure, risk_type='financial'
            )
            operational_risk = self._calculate_synthetic_risk(
                team_size_factor, requirements_clarity, stakeholder_engagement, risk_type='operational'
            )
            team_risk = self._calculate_synthetic_risk(
                team_size_factor, team_experience, timeline_pressure, risk_type='team'
            )
            
            # Classification du niveau de risque global
            overall_risk = np.mean([technical_risk, financial_risk, operational_risk, team_risk])
            if overall_risk < 0.3:
                risk_level = 1  # LOW
            elif overall_risk < 0.6:
                risk_level = 2  # MEDIUM
            elif overall_risk < 0.8:
                risk_level = 3  # HIGH
            else:
                risk_level = 4  # CRITICAL
            
            # Simulation d'impact r√©el (pour l'entra√Ænement)
            actual_impact = overall_risk + np.random.normal(0, 0.1)
            actual_impact = np.clip(actual_impact, 0, 1)
            
            data.append({
                'team_size': team_size,
                'project_duration': project_duration,
                'budget': budget,
                'complexity': complexity,
                'domain': domain,
                'team_experience': team_experience,
                'timeline_pressure': timeline_pressure,
                'requirements_clarity': requirements_clarity,
                'stakeholder_engagement': stakeholder_engagement,
                'technical_risk': technical_risk,
                'financial_risk': financial_risk,
                'operational_risk': operational_risk,
                'team_risk': team_risk,
                'overall_risk_score': overall_risk,
                'risk_level': risk_level,
                'actual_impact': actual_impact
            })
        
        return pd.DataFrame(data)
    
    def _calculate_synthetic_risk(self, *factors, risk_type: str) -> float:
        """Calcule un score de risque synth√©tique"""
        
        # Conversion des facteurs cat√©goriels
        processed_factors = []
        for factor in factors:
            if isinstance(factor, str):
                if factor in ['low', 'junior']:
                    processed_factors.append(0.3)
                elif factor in ['medium', 'mixed']:
                    processed_factors.append(0.6)
                elif factor in ['high', 'senior']:
                    processed_factors.append(0.8)
                else:  # very_high, expert
                    processed_factors.append(1.0)
            else:
                processed_factors.append(float(factor))
        
        # Calcul pond√©r√© selon le type de risque
        weights = {
            'technical': [0.4, 0.3, 0.3],
            'financial': [0.5, 0.3, 0.2],
            'operational': [0.3, 0.4, 0.3],
            'team': [0.3, 0.4, 0.3]
        }
        
        risk_weights = weights.get(risk_type, [1/len(processed_factors)] * len(processed_factors))
        
        # Assurer la compatibilit√© des tailles
        min_len = min(len(processed_factors), len(risk_weights))
        risk_score = np.average(processed_factors[:min_len], weights=risk_weights[:min_len])
        
        # Ajout de bruit r√©aliste
        risk_score += np.random.normal(0, 0.05)
        return np.clip(risk_score, 0, 1)
    
    def _train_initial_models(self):
        """Entra√Æne les mod√®les initiaux avec les donn√©es synth√©tiques"""
        
        df = self.historical_data
        
        # Pr√©paration des features
        feature_columns = [
            'team_size', 'project_duration', 'budget', 'timeline_pressure',
            'requirements_clarity', 'stakeholder_engagement'
        ]
        
        # Encodage des variables cat√©gorielles
        categorical_columns = ['complexity', 'domain', 'team_experience']
        
        for col in categorical_columns:
            le = LabelEncoder()
            df[f'{col}_encoded'] = le.fit_transform(df[col])
            self.feature_encoders[col] = le
            feature_columns.append(f'{col}_encoded')
        
        # Normalisation des features
        X = df[feature_columns].values
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        self.scalers['features'] = scaler
        
        # Entra√Ænement du classificateur de niveau de risque
        y_risk_level = df['risk_level'].values
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_risk_level, test_size=0.2, random_state=42)
        
        self.models['risk_classifier'].fit(X_train, y_train)
        
        # Entra√Ænement du pr√©dicteur d'impact
        y_impact = df['actual_impact'].values
        X_train_imp, X_test_imp, y_train_imp, y_test_imp = train_test_split(X_scaled, y_impact, test_size=0.2, random_state=42)
        
        self.models['impact_predictor'].fit(X_train_imp, y_train_imp)
        
        # Entra√Ænement du pr√©dicteur de probabilit√©
        y_prob = df['overall_risk_score'].values
        X_train_prob, X_test_prob, y_train_prob, y_test_prob = train_test_split(X_scaled, y_prob, test_size=0.2, random_state=42)
        self.models['probability_predictor'].fit(X_train_prob, y_train_prob)
        
        self.trained = True
        
        print(f"[OK] Mod√®les entra√Æn√©s avec {len(df)} projets synth√©tiques")
        print(f"[STATS] Accuracy classificateur: {self.models['risk_classifier'].score(X_test, y_test):.3f}")
        print(f"[METRICS] RMSE pr√©dicteur impact: {np.sqrt(mean_squared_error(y_test_imp, self.models['impact_predictor'].predict(X_test_imp))):.3f}")
    
    async def analyze_project_risks(self, project_data: Dict[str, Any]) -> RiskProfile:
        """Analyse compl√®te des risques d'un projet"""
        
        if not self.trained:
            raise Exception("Mod√®les non entra√Æn√©s. Appelez _train_initial_models() d'abord.")
        
        # Extraction et pr√©paration des features
        features = self._extract_project_features(project_data)
        
        # Pr√©dictions ML
        ml_predictions = await self._run_ml_risk_predictions(features)
        
        # Analyse des patterns
        pattern_risks = self._analyze_risk_patterns(features)
        
        # D√©tection d'anomalies
        anomaly_risks = self._detect_risk_anomalies(features)
        
        # Combinaison des analyses
        predicted_risks = self._combine_risk_analyses(ml_predictions, pattern_risks, anomaly_risks)
        
        # G√©n√©ration du profil complet
        risk_profile = await self._generate_risk_profile(project_data, predicted_risks, features)
        
        return risk_profile
    
    def _extract_project_features(self, project_data: Dict[str, Any]) -> Dict[str, Any]:
        """Extrait les features pertinentes du projet pour l'analyse de risques"""
        
        overview = project_data.get('project_overview', {})
        resources = project_data.get('resources', [])
        phases = project_data.get('phases', [])
        tasks = project_data.get('tasks', [])
        
        # Features de base
        features = {
            'team_size': len(resources),
            'project_duration': overview.get('total_duration', 90),
            'budget': overview.get('total_cost', 100000),
            'complexity': project_data.get('complexity', 'medium'),
            'domain': project_data.get('domain', 'web_app'),
            'team_experience': project_data.get('team_experience', 'mixed'),
            'timeline_pressure': project_data.get('timeline_pressure', 0.5),
            'requirements_clarity': project_data.get('requirements_clarity', 0.7),
            'stakeholder_engagement': project_data.get('stakeholder_engagement', 0.6)
        }
        
        # Features d√©riv√©es
        features.update({
            'phase_count': len(phases),
            'task_count': len(tasks),
            'avg_phase_duration': np.mean([p.get('duration', 30) for p in phases]) if phases else 30,
            'budget_per_person': features['budget'] / max(1, features['team_size']),
            'task_complexity_avg': np.mean([self._estimate_task_complexity(task) for task in tasks]) if tasks else 0.5
        })
        
        # Features contextuelles
        features.update({
            'has_external_dependencies': len(project_data.get('integrations', [])) > 0,
            'regulatory_requirements': 'fintech' in features['domain'] or 'healthcare' in features['domain'],
            'new_technology_ratio': project_data.get('new_tech_ratio', 0.3),
            'geographic_distribution': project_data.get('geographic_distribution', 1)
        })
        
        return features
    
    def _estimate_task_complexity(self, task: Dict[str, Any]) -> float:
        """Estime la complexit√© d'une t√¢che"""
        
        complexity_indicators = {
            'integration': 0.8,
            'api': 0.7,
            'database': 0.6,
            'security': 0.9,
            'performance': 0.8,
            'ui/ux': 0.5,
            'testing': 0.4
        }
        
        task_description = task.get('description', '').lower()
        complexity_scores = []
        
        for indicator, score in complexity_indicators.items():
            if indicator in task_description:
                complexity_scores.append(score)
        
        return np.mean(complexity_scores) if complexity_scores else 0.5
    
    async def _run_ml_risk_predictions(self, features: Dict[str, Any]) -> Dict[str, Any]:
        """Ex√©cute les pr√©dictions ML sur les features"""
        
        # Pr√©paration des features pour les mod√®les
        feature_vector = self._prepare_feature_vector(features)
        
        # Pr√©dictions
        risk_level_pred = self.models['risk_classifier'].predict([feature_vector])[0]
        risk_level_proba = self.models['risk_classifier'].predict_proba([feature_vector])[0]
        
        impact_pred = self.models['impact_predictor'].predict([feature_vector])[0]
        probability_pred = self.models['probability_predictor'].predict([feature_vector])[0]
        
        # Pr√©dictions par cat√©gorie (simul√©es - √† am√©liorer avec mod√®les sp√©cialis√©s)
        category_predictions = {}
        for category in RiskCategory:
            # Ajustement bas√© sur les caract√©ristiques du projet
            base_score = probability_pred
            
            if category == RiskCategory.TECHNICAL:
                base_score *= (1 + features.get('task_complexity_avg', 0.5))
            elif category == RiskCategory.FINANCIAL:
                base_score *= (1 + features.get('timeline_pressure', 0.5))
            elif category == RiskCategory.TEAM:
                team_factor = min(2.0, features.get('team_size', 5) / 10)
                base_score *= (1 + team_factor - 1)
            elif category == RiskCategory.OPERATIONAL:
                base_score *= (1 + (1 - features.get('requirements_clarity', 0.7)))
            
            category_predictions[category] = np.clip(base_score, 0, 1)
        
        return {
            'overall_risk_level': risk_level_pred,
            'risk_level_probabilities': risk_level_proba,
            'predicted_impact': impact_pred,
            'predicted_probability': probability_pred,
            'category_predictions': category_predictions,
            'confidence': np.max(risk_level_proba)  # Confiance = probabilit√© max
        }
    
    def _prepare_feature_vector(self, features: Dict[str, Any]) -> np.ndarray:
        """Pr√©pare le vecteur de features pour les mod√®les ML"""
        
        # Features num√©riques
        numeric_features = [
            features.get('team_size', 5),
            features.get('project_duration', 90),
            features.get('budget', 100000),
            features.get('timeline_pressure', 0.5),
            features.get('requirements_clarity', 0.7),
            features.get('stakeholder_engagement', 0.6)
        ]
        
        # Encodage des features cat√©gorielles
        categorical_features = []
        for cat_feature in ['complexity', 'domain', 'team_experience']:
            if cat_feature in self.feature_encoders:
                try:
                    encoded_value = self.feature_encoders[cat_feature].transform([features.get(cat_feature, 'medium')])[0]
                    categorical_features.append(encoded_value)
                except ValueError:
                    # Valeur inconnue, utiliser la valeur moyenne
                    categorical_features.append(0)
        
        # Combinaison
        feature_vector = np.array(numeric_features + categorical_features)
        
        # Normalisation
        if 'features' in self.scalers:
            feature_vector = self.scalers['features'].transform([feature_vector])[0]
        
        return feature_vector
    
    def _analyze_risk_patterns(self, features: Dict[str, Any]) -> List[PredictedRisk]:
        """Analyse les risques bas√©s sur les patterns connus"""
        
        pattern_risks = []
        project_patterns = self.risk_patterns['project_patterns']
        
        for pattern_name, pattern_info in project_patterns.items():
            # V√©rification des conditions de d√©clenchement
            triggers_met = []
            
            for condition_key, (operator, threshold) in pattern_info['trigger_conditions'].items():
                feature_value = features.get(condition_key, 0)
                
                if operator == '>' and feature_value > threshold:
                    triggers_met.append(f"{condition_key} > {threshold}")
                elif operator == '<' and feature_value < threshold:
                    triggers_met.append(f"{condition_key} < {threshold}")
                elif operator == '=' and feature_value == threshold:
                    triggers_met.append(f"{condition_key} = {threshold}")
            
            # Si des triggers sont activ√©s, cr√©er le risque pr√©dit
            if triggers_met:
                risk_id = f"pattern_{pattern_name}_{datetime.now().timestamp()}"
                
                # Calcul du niveau de risque bas√© sur la probabilit√© historique
                historical_prob = pattern_info['historical_probability']
                avg_impact = pattern_info['avg_impact']
                
                if historical_prob * avg_impact > 0.7:
                    risk_level = RiskLevel.CRITICAL
                elif historical_prob * avg_impact > 0.5:
                    risk_level = RiskLevel.HIGH
                elif historical_prob * avg_impact > 0.3:
                    risk_level = RiskLevel.MEDIUM
                else:
                    risk_level = RiskLevel.LOW
                
                # D√©termination de la cat√©gorie principale
                category = self._determine_risk_category(pattern_info['risk_factors'])
                
                # G√©n√©ration des strat√©gies de mitigation
                mitigation_strategies = self._generate_mitigation_strategies(pattern_name, pattern_info)
                
                pattern_risk = PredictedRisk(
                    risk_id=risk_id,
                    name=pattern_name.replace('_', ' ').title(),
                    category=category,
                    predicted_level=risk_level,
                    probability=historical_prob,
                    potential_impact=avg_impact,
                    confidence=0.8,  # Haute confiance pour les patterns √©tablis
                    triggers=triggers_met,
                    early_warnings=self._generate_early_warnings(pattern_name),
                    mitigation_strategies=mitigation_strategies,
                    similar_cases=[],  # √Ä impl√©menter avec vraie base de donn√©es
                    predicted_timeline=None,
                    prevention_actions=self._generate_prevention_actions(pattern_name)
                )
                
                pattern_risks.append(pattern_risk)
        
        return pattern_risks
    
    def _determine_risk_category(self, risk_factors: List[str]) -> RiskCategory:
        """D√©termine la cat√©gorie principale d'un risque bas√© sur ses facteurs"""
        
        category_keywords = {
            RiskCategory.TECHNICAL: ['technical', 'integration', 'code', 'architecture', 'performance'],
            RiskCategory.TEAM: ['team', 'communication', 'coordination', 'turnover', 'skill'],
            RiskCategory.FINANCIAL: ['budget', 'cost', 'funding', 'financial'],
            RiskCategory.OPERATIONAL: ['process', 'workflow', 'operational', 'resource'],
            RiskCategory.STRATEGIC: ['strategic', 'business', 'market', 'competition']
        }
        
        category_scores = {}
        
        for category, keywords in category_keywords.items():
            score = 0
            for factor in risk_factors:
                factor_lower = factor.lower()
                for keyword in keywords:
                    if keyword in factor_lower:
                        score += 1
            category_scores[category] = score
        
        # Retourner la cat√©gorie avec le score le plus √©lev√©
        if category_scores:
            return max(category_scores, key=category_scores.get)
        
        return RiskCategory.OPERATIONAL  # D√©faut
    
    def _generate_mitigation_strategies(self, pattern_name: str, pattern_info: Dict[str, Any]) -> List[str]:
        """G√©n√®re des strat√©gies de mitigation sp√©cifiques"""
        
        mitigation_templates = {
            'large_team_communication': [
                "Mettre en place des √©quipes autonomes de 5-7 personnes maximum",
                "Utiliser des outils de communication asynchrone (Slack, Teams)",
                "Organiser des daily standups courts et focalis√©s",
                "Cr√©er une documentation partag√©e et maintenue √† jour"
            ],
            'aggressive_timeline': [
                "N√©gocier un scope r√©duit avec les fonctionnalit√©s critiques seulement",
                "Impl√©menter une approche MVP (Minimum Viable Product)",
                "Augmenter temporairement les ressources sur les t√¢ches critiques",
                "Mettre en place des processus de d√©veloppement parall√®le"
            ],
            'new_technology_stack': [
                "Pr√©voir du temps pour formation et mont√©e en comp√©tences",
                "Cr√©er des POCs (Proof of Concepts) pour valider l'approche",
                "Mettre en place un mentorat avec des experts externes",
                "Avoir un plan de fallback vers des technologies connues"
            ],
            'distributed_team': [
                "D√©finir des cr√©neaux de chevauchement obligatoires",
                "Utiliser des outils de collaboration temps r√©el",
                "Mettre en place des processus de handover structur√©s",
                "Organiser des r√©unions d'√©quipe r√©guli√®res par vid√©oconf√©rence"
            ]
        }
        
        return mitigation_templates.get(pattern_name, [
            "Surveiller de pr√®s l'√©volution de cette situation",
            "Mettre en place des m√©triques de suivi sp√©cifiques",
            "Pr√©parer un plan de contingence",
            "Communiquer r√©guli√®rement avec les stakeholders"
        ])
    
    def _generate_early_warnings(self, pattern_name: str) -> List[str]:
        """G√©n√®re les signaux d'alerte pr√©coce"""
        
        warning_templates = {
            'large_team_communication': [
                "Augmentation du temps pass√© en r√©unions",
                "Baisse de la v√©locit√© des sprints",
                "Augmentation des conflits de merge",
                "Retards dans les livrables inter-√©quipes"
            ],
            'aggressive_timeline': [
                "Accumulation de dette technique",
                "Augmentation du nombre de bugs",
                "Baisse de la couverture de tests",
                "Signaux de stress dans l'√©quipe"
            ],
            'new_technology_stack': [
                "Temps de d√©veloppement plus long que pr√©vu",
                "Multiplication des questions techniques",
                "Difficult√©s d'int√©gration",
                "Besoin fr√©quent de support externe"
            ],
            'distributed_team': [
                "Retards dans la communication d'informations critiques",
                "Duplication de travail entre √©quipes",
                "Probl√®mes de coh√©rence dans le code",
                "Frustration exprim√©e par les membres de l'√©quipe"
            ]
        }
        
        return warning_templates.get(pattern_name, [
            "D√©rive des m√©triques cl√©s du projet",
            "Feedback n√©gatif des stakeholders",
            "Augmentation des incidents"
        ])
    
    def _generate_prevention_actions(self, pattern_name: str) -> List[str]:
        """G√©n√®re les actions de pr√©vention"""
        
        prevention_templates = {
            'large_team_communication': [
                "Organiser des ateliers de team building",
                "D√©finir clairement les r√¥les et responsabilit√©s",
                "Mettre en place une matrice RACI",
                "Former les leads aux techniques de facilitation"
            ],
            'aggressive_timeline': [
                "Valider le scope avec toutes les parties prenantes",
                "√âtablir des jalons interm√©diaires avec validation",
                "Pr√©voir des buffers de temps pour les impr√©vus",
                "Mettre en place un monitoring de la v√©locit√©"
            ],
            'new_technology_stack': [
                "Faire un audit des comp√©tences disponibles",
                "Organiser des sessions de formation pr√©ventives",
                "Cr√©er une veille technologique active",
                "√âtablir des partenariats avec des experts"
            ],
            'distributed_team': [
                "D√©finir des standards de communication",
                "Mettre en place des outils collaboratifs performants",
                "Organiser des rencontres physiques r√©guli√®res",
                "Cr√©er une culture d'√©quipe forte"
            ]
        }
        
        return prevention_templates.get(pattern_name, [
            "Effectuer des audits pr√©ventifs r√©guliers",
            "Maintenir une communication proactive",
            "Surveiller les indicateurs de performance"
        ])
    
    def _detect_risk_anomalies(self, features: Dict[str, Any]) -> List[PredictedRisk]:
        """D√©tecte les anomalies qui pourraient indiquer des risques cach√©s"""
        
        anomaly_risks = []
        
        # D√©tection d'anomalies dans les ratios
        budget_per_person = features.get('budget_per_person', 20000)
        if budget_per_person < 10000:
            anomaly_risks.append(self._create_anomaly_risk(
                "Budget per person trop faible",
                RiskCategory.FINANCIAL,
                f"Budget par personne: {budget_per_person:,.0f}‚Ç¨ (recommand√©: >20k‚Ç¨)",
                0.7
            ))
        
        timeline_pressure = features.get('timeline_pressure', 0.5)
        team_size = features.get('team_size', 5)
        if timeline_pressure > 0.8 and team_size < 5:
            anomaly_risks.append(self._create_anomaly_risk(
                "√âquipe insuffisante pour timeline agressive",
                RiskCategory.OPERATIONAL,
                f"Pression temporelle {timeline_pressure:.1%} avec seulement {team_size} personnes",
                0.8
            ))
        
        # Anomalie de complexit√© vs exp√©rience
        complexity = features.get('complexity', 'medium')
        team_experience = features.get('team_experience', 'mixed')
        
        high_complexity = complexity in ['high', 'very_high']
        low_experience = team_experience in ['junior', 'mixed']
        
        if high_complexity and low_experience:
            anomaly_risks.append(self._create_anomaly_risk(
                "Complexit√© √©lev√©e avec √©quipe inexp√©riment√©e",
                RiskCategory.TEAM,
                f"Projet {complexity} avec √©quipe {team_experience}",
                0.9
            ))
        
        return anomaly_risks
    
    def _create_anomaly_risk(self, name: str, category: RiskCategory, 
                           description: str, severity: float) -> PredictedRisk:
        """Cr√©e un risque d√©tect√© par anomalie"""
        
        if severity > 0.8:
            risk_level = RiskLevel.HIGH
        elif severity > 0.6:
            risk_level = RiskLevel.MEDIUM
        else:
            risk_level = RiskLevel.LOW
        
        return PredictedRisk(
            risk_id=f"anomaly_{datetime.now().timestamp()}",
            name=name,
            category=category,
            predicted_level=risk_level,
            probability=severity,
            potential_impact=severity,
            confidence=0.6,  # Moyenne confiance pour anomalies
            triggers=[f"D√©tection automatique: {description}"],
            early_warnings=["M√©triques inhabituelles d√©tect√©es"],
            mitigation_strategies=[
                "Analyser les causes de cette anomalie",
                "Ajuster les param√®tres du projet si n√©cessaire",
                "Surveiller de pr√®s l'√©volution"
            ],
            similar_cases=[],
            predicted_timeline=None,
            prevention_actions=["Validation des m√©triques de projet"]
        )
    
    def _combine_risk_analyses(self, ml_predictions: Dict[str, Any], 
                             pattern_risks: List[PredictedRisk],
                             anomaly_risks: List[PredictedRisk]) -> List[PredictedRisk]:
        """Combine les diff√©rentes analyses de risque"""
        
        combined_risks = pattern_risks + anomaly_risks
        
        # Ajout des risques g√©n√©riques bas√©s sur les pr√©dictions ML
        category_predictions = ml_predictions['category_predictions']
        
        for category, prediction_score in category_predictions.items():
            if prediction_score > 0.5:  # Seuil de pertinence
                
                if prediction_score > 0.8:
                    level = RiskLevel.HIGH
                elif prediction_score > 0.6:
                    level = RiskLevel.MEDIUM
                else:
                    level = RiskLevel.LOW
                
                generic_risk = PredictedRisk(
                    risk_id=f"ml_{category.value}_{datetime.now().timestamp()}",
                    name=f"Risque {category.value.replace('_', ' ').title()}",
                    category=category,
                    predicted_level=level,
                    probability=prediction_score,
                    potential_impact=ml_predictions['predicted_impact'],
                    confidence=ml_predictions['confidence'],
                    triggers=["Pr√©diction bas√©e sur mod√®le ML"],
                    early_warnings=self._get_generic_warnings(category),
                    mitigation_strategies=self._get_generic_mitigations(category),
                    similar_cases=[],
                    predicted_timeline=None,
                    prevention_actions=self._get_generic_preventions(category)
                )
                
                combined_risks.append(generic_risk)
        
        # D√©duplication et tri par priorit√©
        unique_risks = self._deduplicate_risks(combined_risks)
        sorted_risks = sorted(unique_risks, key=lambda r: r.probability * r.potential_impact, reverse=True)
        
        return sorted_risks[:20]  # Limite √† 20 risques principaux
    
    def _get_generic_warnings(self, category: RiskCategory) -> List[str]:
        """Retourne les signaux d'alerte g√©n√©riques par cat√©gorie"""
        
        warnings_map = {
            RiskCategory.TECHNICAL: [
                "Augmentation du temps de build",
                "Multiplication des bugs",
                "Difficult√©s d'int√©gration"
            ],
            RiskCategory.FINANCIAL: [
                "D√©passement budg√©taire partiel",
                "Co√ªts cach√©s √©mergents",
                "Demandes de ressources suppl√©mentaires"
            ],
            RiskCategory.TEAM: [
                "Baisse de moral de l'√©quipe",
                "Turnover en augmentation",
                "Conflits interpersonnels"
            ],
            RiskCategory.OPERATIONAL: [
                "Processus inefficaces",
                "Goulots d'√©tranglement r√©currents",
                "Retards dans les livrables"
            ]
        }
        
        return warnings_map.get(category, ["Signaux d'alerte √† surveiller"])
    
    def _get_generic_mitigations(self, category: RiskCategory) -> List[str]:
        """Retourne les mitigations g√©n√©riques par cat√©gorie"""
        
        mitigations_map = {
            RiskCategory.TECHNICAL: [
                "Renforcer les revues de code",
                "Am√©liorer la couverture de tests",
                "Mettre en place une int√©gration continue robuste"
            ],
            RiskCategory.FINANCIAL: [
                "Surveiller le budget hebdomadairement",
                "N√©gocier avec les fournisseurs",
                "Optimiser l'allocation des ressources"
            ],
            RiskCategory.TEAM: [
                "Organiser des sessions de team building",
                "Am√©liorer la communication interne",
                "Mettre en place un syst√®me de feedback"
            ],
            RiskCategory.OPERATIONAL: [
                "Optimiser les processus existants",
                "Automatiser les t√¢ches r√©p√©titives",
                "Am√©liorer la coordination inter-√©quipes"
            ]
        }
        
        return mitigations_map.get(category, ["Strat√©gies de mitigation √† d√©finir"])
    
    def _get_generic_preventions(self, category: RiskCategory) -> List[str]:
        """Retourne les actions pr√©ventives g√©n√©riques par cat√©gorie"""
        
        preventions_map = {
            RiskCategory.TECHNICAL: [
                "Effectuer des audits techniques r√©guliers",
                "Maintenir une documentation technique √† jour",
                "Organiser des formations techniques"
            ],
            RiskCategory.FINANCIAL: [
                "√âtablir un suivi budg√©taire rigoureux",
                "Pr√©voir des contingences financi√®res",
                "Valider tous les co√ªts avec la direction"
            ],
            RiskCategory.TEAM: [
                "Maintenir une communication ouverte",
                "Organiser des one-to-one r√©guliers",
                "Surveiller la charge de travail"
            ],
            RiskCategory.OPERATIONAL: [
                "Documenter tous les processus",
                "Effectuer des r√©trospectives r√©guli√®res",
                "Optimiser continuellement les workflows"
            ]
        }
        
        return preventions_map.get(category, ["Actions pr√©ventives √† d√©finir"])
    
    def _deduplicate_risks(self, risks: List[PredictedRisk]) -> List[PredictedRisk]:
        """Supprime les risques en doublon"""
        
        seen_signatures = set()
        unique_risks = []
        
        for risk in risks:
            # Signature bas√©e sur nom + cat√©gorie
            signature = f"{risk.name.lower()}_{risk.category.value}"
            
            if signature not in seen_signatures:
                seen_signatures.add(signature)
                unique_risks.append(risk)
            else:
                # Si risque similaire existe, garder celui avec la plus haute probabilit√©
                for i, existing_risk in enumerate(unique_risks):
                    existing_signature = f"{existing_risk.name.lower()}_{existing_risk.category.value}"
                    if existing_signature == signature:
                        if risk.probability > existing_risk.probability:
                            unique_risks[i] = risk
                        break
        
        return unique_risks
    
    async def _generate_risk_profile(self, project_data: Dict[str, Any], 
                                   predicted_risks: List[PredictedRisk],
                                   features: Dict[str, Any]) -> RiskProfile:
        """G√©n√®re le profil de risque complet"""
        
        # Score de risque global
        if predicted_risks:
            risk_scores = [r.probability * r.potential_impact for r in predicted_risks]
            overall_risk_score = np.mean(risk_scores)
        else:
            overall_risk_score = 0.3  # Risque de base
        
        # Distribution par cat√©gorie
        risk_distribution = {}
        for category in RiskCategory:
            category_risks = [r for r in predicted_risks if r.category == category]
            if category_risks:
                avg_score = np.mean([r.probability * r.potential_impact for r in category_risks])
                risk_distribution[category] = avg_score
            else:
                risk_distribution[category] = 0.0
        
        # Tendances de risque (simul√©es - √† am√©liorer avec donn√©es historiques)
        risk_trends = {}
        for category in RiskCategory:
            # Simulation d'√©volution sur 12 semaines
            base_value = risk_distribution[category]
            trend = [base_value + np.random.normal(0, 0.05) for _ in range(12)]
            risk_trends[category.value] = [max(0, min(1, value)) for value in trend]
        
        # Risques sur le chemin critique (simplifi√©s)
        critical_path_risks = [
            risk.name for risk in predicted_risks[:5]
            if risk.predicted_level in [RiskLevel.HIGH, RiskLevel.CRITICAL]
        ]
        
        # Efficacit√© des mitigations (simul√©e)
        mitigation_effectiveness = {}
        for risk in predicted_risks[:10]:
            effectiveness = np.random.uniform(0.6, 0.9)  # √Ä remplacer par donn√©es r√©elles
            mitigation_effectiveness[risk.name] = effectiveness
        
        # Intervalles de confiance
        confidence_intervals = {}
        for category in RiskCategory:
            base_score = risk_distribution[category]
            margin = 0.1 * (1 - features.get('requirements_clarity', 0.7))  # Plus d'incertitude si requirements flous
            confidence_intervals[category.value] = (
                max(0, base_score - margin),
                min(1, base_score + margin)
            )
        
        return RiskProfile(
            project_id=project_data.get('project_id', 'unknown'),
            overall_risk_score=overall_risk_score,
            risk_distribution=risk_distribution,
            predicted_risks=predicted_risks,
            risk_trends=risk_trends,
            critical_path_risks=critical_path_risks,
            mitigation_effectiveness=mitigation_effectiveness,
            confidence_intervals=confidence_intervals,
            generated_at=datetime.now()
        )
    
    def update_model_with_feedback(self, project_id: str, actual_risks: List[Dict[str, Any]]):
        """Met √† jour les mod√®les avec les retours d'exp√©rience r√©els"""
        
        # Cette m√©thode permettrait d'am√©liorer les mod√®les avec les donn√©es r√©elles
        # √Ä impl√©menter avec un syst√®me de feedback structur√©
        pass
    
    def export_risk_report(self, risk_profile: RiskProfile) -> Dict[str, Any]:
        """Exporte un rapport de risques d√©taill√©"""
        
        report = {
            'executive_summary': {
                'project_id': risk_profile.project_id,
                'overall_risk_level': self._get_risk_level_text(risk_profile.overall_risk_score),
                'total_risks_identified': len(risk_profile.predicted_risks),
                'critical_risks_count': len([r for r in risk_profile.predicted_risks if r.predicted_level == RiskLevel.CRITICAL]),
                'generated_at': risk_profile.generated_at.isoformat()
            },
            'risk_breakdown': {
                category.value: {
                    'score': score,
                    'level': self._get_risk_level_text(score),
                    'confidence_interval': risk_profile.confidence_intervals.get(category.value, (0, 1))
                }
                for category, score in risk_profile.risk_distribution.items()
            },
            'top_risks': [
                {
                    'name': risk.name,
                    'category': risk.category.value,
                    'level': risk.predicted_level.name,
                    'probability': risk.probability,
                    'impact': risk.potential_impact,
                    'confidence': risk.confidence,
                    'mitigation_strategies': risk.mitigation_strategies
                }
                for risk in risk_profile.predicted_risks[:10]
            ],
            'recommendations': self._generate_strategic_recommendations(risk_profile),
            'action_plan': self._generate_action_plan(risk_profile)
        }
        
        return report
    
    def _get_risk_level_text(self, risk_score: float) -> str:
        """Convertit un score de risque en texte descriptif"""
        
        if risk_score >= 0.8:
            return "CRITIQUE"
        elif risk_score >= 0.6:
            return "√âLEV√â"
        elif risk_score >= 0.4:
            return "MOD√âR√â"
        elif risk_score >= 0.2:
            return "FAIBLE"
        else:
            return "TR√àS FAIBLE"
    
    def _generate_strategic_recommendations(self, risk_profile: RiskProfile) -> List[str]:
        """G√©n√®re des recommandations strat√©giques bas√©es sur le profil de risque"""
        
        recommendations = []
        
        # Recommandations bas√©es sur le score global
        if risk_profile.overall_risk_score > 0.7:
            recommendations.append("üö® URGENT: Revoir imm√©diatement la faisabilit√© du projet")
            recommendations.append("üìã Consid√©rer une r√©duction du scope pour limiter l'exposition aux risques")
            
        # Recommandations par cat√©gorie dominante
        max_risk_category = max(risk_profile.risk_distribution.items(), key=lambda x: x[1])
        
        if max_risk_category[1] > 0.6:
            category_recommendations = {
                RiskCategory.TECHNICAL: "üîß Renforcer l'expertise technique et pr√©voir plus de temps pour R&D",
                RiskCategory.FINANCIAL: "üí∞ R√©viser le budget et n√©gocier des marges de s√©curit√©",
                RiskCategory.TEAM: "üë• Investir dans la coh√©sion d'√©quipe et la formation",
                RiskCategory.OPERATIONAL: "‚öôÔ∏è Optimiser les processus et am√©liorer la coordination"
            }
            
            rec = category_recommendations.get(max_risk_category[0])
            if rec:
                recommendations.append(rec)
        
        # Recommandations sur les risques critiques
        critical_risks = [r for r in risk_profile.predicted_risks if r.predicted_level == RiskLevel.CRITICAL]
        if critical_risks:
            recommendations.append(f"‚ö†Ô∏è Traiter en priorit√© les {len(critical_risks)} risques critiques identifi√©s")
        
        return recommendations[:5]  # Top 5 recommandations
    
    def _generate_action_plan(self, risk_profile: RiskProfile) -> List[Dict[str, Any]]:
        """G√©n√®re un plan d'action structur√©"""
        
        action_plan = []
        
        # Actions imm√©diates (risques critiques)
        critical_risks = [r for r in risk_profile.predicted_risks if r.predicted_level == RiskLevel.CRITICAL]
        
        for risk in critical_risks[:3]:  # Top 3 risques critiques
            action_plan.append({
                'priority': 'URGENT',
                'action': f"Mitiger le risque: {risk.name}",
                'strategies': risk.mitigation_strategies,
                'timeline': '1-2 semaines',
                'responsible': 'Chef de projet + √©quipe concern√©e'
            })
        
        # Actions √† court terme (risques √©lev√©s)
        high_risks = [r for r in risk_profile.predicted_risks if r.predicted_level == RiskLevel.HIGH]
        
        for risk in high_risks[:5]:  # Top 5 risques √©lev√©s
            action_plan.append({
                'priority': 'HAUTE',
                'action': f"Pr√©venir le risque: {risk.name}",
                'strategies': risk.prevention_actions,
                'timeline': '2-4 semaines',
                'responsible': '√âquipe projet'
            })
        
        # Actions de surveillance (tous les autres)
        action_plan.append({
            'priority': 'CONTINUE',
            'action': 'Surveiller l\'√©volution des risques identifi√©s',
            'strategies': ['Monitoring hebdomadaire', 'Mise √† jour des m√©triques', 'R√©unions de suivi'],
            'timeline': 'Tout au long du projet',
            'responsible': 'Project Manager'
        })
        
        return action_plan


# Instance globale
ai_risk_predictor = AIRiskPredictor()