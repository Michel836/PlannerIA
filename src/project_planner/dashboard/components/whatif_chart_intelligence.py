"""
üß† Intelligence Graphique Avanc√©e pour What-If Analysis
Syst√®me r√©volutionnaire d'analyse et d'interpr√©tation automatique des graphiques
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.figure_factory as ff
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass
import json
try:
    import scipy.stats as stats
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
try:
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
import warnings
warnings.filterwarnings('ignore')

@dataclass
class ChartInsight:
    """Insight automatique g√©n√©r√© par IA"""
    type: str  # trend, outlier, correlation, pattern, risk, opportunity
    title: str
    description: str
    confidence: float  # 0-1
    severity: str  # low, medium, high, critical
    action_items: List[str]
    supporting_data: Dict[str, Any]

@dataclass
class ChartAnalysis:
    """Analyse compl√®te d'un graphique"""
    chart_type: str
    insights: List[ChartInsight]
    statistical_summary: Dict[str, Any]
    business_interpretation: str
    recommendations: List[str]

class IntelligentChartAnalyzer:
    """Analyseur intelligent de graphiques avec IA"""
    
    def __init__(self):
        self.analysis_cache = {}
        self.insight_patterns = self._load_insight_patterns()
        
    def _load_insight_patterns(self) -> Dict[str, Any]:
        """Patterns d'insights pr√©-configur√©s"""
        return {
            'monte_carlo_patterns': {
                'high_variance': {
                    'threshold': 0.4,
                    'message': "Variance √©lev√©e d√©tect√©e - Projet √† haut risque",
                    'severity': 'high'
                },
                'skewness': {
                    'threshold': 1.5,
                    'message': "Distribution asym√©trique - Risque de d√©passement",
                    'severity': 'medium'
                },
                'tail_risk': {
                    'threshold': 0.1,
                    'message': "Risque de queue important - Sc√©narios extr√™mes probables",
                    'severity': 'high'
                }
            },
            'scenario_patterns': {
                'pareto_optimal': {
                    'message': "Zone Pareto-optimale identifi√©e",
                    'severity': 'low'
                },
                'dominated_solutions': {
                    'message': "Solutions domin√©es d√©tect√©es - Optimisation possible",
                    'severity': 'medium'
                }
            },
            'correlation_patterns': {
                'strong_positive': {
                    'threshold': 0.7,
                    'message': "Corr√©lation forte positive d√©tect√©e",
                    'severity': 'medium'
                },
                'negative_correlation': {
                    'threshold': -0.5,
                    'message': "Corr√©lation n√©gative - Trade-off critique",
                    'severity': 'high'
                }
            }
        }
    
    def create_intelligent_monte_carlo_chart(self, simulation_data: Dict[str, Any], 
                                           scenario_params: Dict[str, Any]) -> Tuple[go.Figure, ChartAnalysis]:
        """Cr√©e un graphique Monte Carlo avec analyse intelligente"""
        
        # Extraction ultra-s√©curis√©e des donn√©es avec fallbacks multiples
        try:
            if isinstance(simulation_data, dict):
                if 'simulation_data' in simulation_data:
                    sim_data = simulation_data['simulation_data']
                else:
                    sim_data = simulation_data
                    
                # Extraction avec fallbacks robustes
                durations = np.array(sim_data.get('durations', sim_data.get('duration', [100, 120, 90, 110, 105])))
                costs = np.array(sim_data.get('costs', sim_data.get('cost', [50000, 60000, 45000, 55000, 52000])))
                qualities = np.array(sim_data.get('qualities', sim_data.get('quality', [0.8, 0.75, 0.85, 0.78, 0.82])))
            else:
                # Fallback complet si les donn√©es ne sont pas un dict
                durations = np.array([100, 120, 90, 110, 105])
                costs = np.array([50000, 60000, 45000, 55000, 52000])
                qualities = np.array([0.8, 0.75, 0.85, 0.78, 0.82])
        except Exception as e:
            # Fallback final en cas d'erreur
            durations = np.array([100, 120, 90, 110, 105])
            costs = np.array([50000, 60000, 45000, 55000, 52000])
            qualities = np.array([0.8, 0.75, 0.85, 0.78, 0.82])
        
        # Analyse statistique avanc√©e
        duration_stats = self._calculate_advanced_stats(durations)
        cost_stats = self._calculate_advanced_stats(costs)
        quality_stats = self._calculate_advanced_stats(qualities)
        
        # Cr√©ation du graphique multi-panneaux
        fig = make_subplots(
            rows=2, cols=3,
            subplot_titles=[
                "Distribution Dur√©e (avec Insights)", 
                "Distribution Co√ªt (avec Insights)",
                "Distribution Qualit√©",
                "Corr√©lation Dur√©e-Co√ªt", 
                "Analyse de Risque (VaR)",
                "Zones de Confiance"
            ],
            specs=[
                [{"secondary_y": False}, {"secondary_y": False}, {"secondary_y": False}],
                [{"secondary_y": True}, {"secondary_y": True}, {"secondary_y": False}]
            ]
        )
        
        # 1. Distribution dur√©e avec insights
        fig.add_trace(
            go.Histogram(x=durations, nbinsx=30, name="Dur√©e", 
                        marker_color="rgba(55, 128, 191, 0.7)",
                        hovertemplate="Dur√©e: %{x:.1f}j<br>Fr√©quence: %{y}<extra></extra>"),
            row=1, col=1
        )
        
        # Ajout des seuils critiques
        p95_duration = np.percentile(durations, 95)
        median_duration = np.median(durations)
        
        fig.add_vline(x=median_duration, line_dash="dash", line_color="green", 
                     annotation_text="M√©diane", row=1, col=1)
        fig.add_vline(x=p95_duration, line_dash="dash", line_color="red", 
                     annotation_text="VaR 95%", row=1, col=1)
        
        # 2. Distribution co√ªt avec insights
        fig.add_trace(
            go.Histogram(x=costs, nbinsx=30, name="Co√ªt", 
                        marker_color="rgba(219, 64, 82, 0.7)",
                        hovertemplate="Co√ªt: $%{x:,.0f}<br>Fr√©quence: %{y}<extra></extra>"),
            row=1, col=2
        )
        
        p95_cost = np.percentile(costs, 95)
        median_cost = np.median(costs)
        
        fig.add_vline(x=median_cost, line_dash="dash", line_color="green", 
                     annotation_text="M√©diane", row=1, col=2)
        fig.add_vline(x=p95_cost, line_dash="dash", line_color="red", 
                     annotation_text="Budget Critique", row=1, col=2)
        
        # 3. Distribution qualit√©
        fig.add_trace(
            go.Histogram(x=qualities, nbinsx=20, name="Qualit√©", 
                        marker_color="rgba(50, 171, 96, 0.7)"),
            row=1, col=3
        )
        
        # 4. Corr√©lation dur√©e-co√ªt avec clustering
        correlation_coef = np.corrcoef(durations, costs)[0, 1]
        
        # K-means clustering pour identifier les zones (si sklearn disponible)
        if SKLEARN_AVAILABLE and len(durations) > 10:
            try:
                clustering_data = np.column_stack([durations[:min(100, len(durations))], costs[:min(100, len(costs))]])
                scaler = StandardScaler()
                scaled_data = scaler.fit_transform(clustering_data)
                
                kmeans = KMeans(n_clusters=3, random_state=42)
                clusters = kmeans.fit_predict(scaled_data)
                
                # Couleurs par cluster
                colors = ['red', 'blue', 'green']
                for i in range(3):
                    mask = clusters == i
                    if sum(mask) > 0:  # S'assurer qu'il y a des points dans le cluster
                        fig.add_trace(
                            go.Scatter(
                                x=durations[:len(clustering_data)][mask], 
                                y=costs[:len(clustering_data)][mask],
                                mode='markers',
                                name=f'Cluster {i+1}',
                                marker_color=colors[i],
                                hovertemplate="Dur√©e: %{x:.1f}j<br>Co√ªt: $%{y:,.0f}<br>Cluster: %{text}<extra></extra>",
                                text=[f'Zone {i+1}'] * sum(mask)
                            ),
                            row=2, col=1
                        )
            except Exception as e:
                # Fallback si clustering √©choue
                fig.add_trace(
                    go.Scatter(x=durations, y=costs, mode='markers', name="Projets",
                              marker_color="rgba(128, 0, 128, 0.6)"),
                    row=2, col=1
                )
        else:
            fig.add_trace(
                go.Scatter(x=durations, y=costs, mode='markers', name="Projets",
                          marker_color="rgba(128, 0, 128, 0.6)"),
                row=2, col=1
            )
        
        # Ligne de r√©gression
        z = np.polyfit(durations, costs, 1)
        p = np.poly1d(z)
        fig.add_trace(
            go.Scatter(x=sorted(durations), y=p(sorted(durations)), 
                      mode='lines', name=f'Tendance (r={correlation_coef:.2f})',
                      line_color="orange"),
            row=2, col=1
        )
        
        # 5. Analyse VaR multi-niveaux
        var_levels = [90, 95, 99]
        var_durations = [np.percentile(durations, level) for level in var_levels]
        var_costs = [np.percentile(costs, level) for level in var_levels]
        
        fig.add_trace(
            go.Bar(x=[f"VaR {level}%" for level in var_levels], 
                  y=var_durations, name="Dur√©e VaR", 
                  marker_color="lightblue", yaxis="y2"),
            row=2, col=2
        )
        
        fig.add_trace(
            go.Bar(x=[f"VaR {level}%" for level in var_levels], 
                  y=var_costs, name="Co√ªt VaR", 
                  marker_color="lightcoral"),
            row=2, col=2, secondary_y=True
        )
        
        # 6. Zones de confiance (contour plot)
        if len(durations) > 50:
            # Cr√©ation d'une heatmap de densit√©
            hist, x_edges, y_edges = np.histogram2d(durations[:100], costs[:100], bins=15)
            
            fig.add_trace(
                go.Heatmap(
                    z=hist.T,
                    x=x_edges[:-1],
                    y=y_edges[:-1],
                    colorscale="Viridis",
                    name="Densit√©",
                    hovertemplate="Dur√©e: %{x:.1f}j<br>Co√ªt: $%{y:,.0f}<br>Densit√©: %{z}<extra></extra>"
                ),
                row=2, col=3
            )
        
        # Mise en forme avanc√©e
        fig.update_layout(
            title="üß† Analyse Monte Carlo Intelligente - Insights Automatiques",
            height=800,
            showlegend=True,
            template="plotly_white"
        )
        
        # G√©n√©ration de l'analyse
        insights = self._generate_monte_carlo_insights(
            duration_stats, cost_stats, quality_stats, correlation_coef
        )
        
        analysis = ChartAnalysis(
            chart_type="intelligent_monte_carlo",
            insights=insights,
            statistical_summary={
                "duration_stats": duration_stats,
                "cost_stats": cost_stats,
                "correlation": correlation_coef,
                "sample_size": len(durations)
            },
            business_interpretation=self._generate_business_interpretation(insights),
            recommendations=self._generate_recommendations(insights)
        )
        
        return fig, analysis
    
    def create_intelligent_pareto_chart(self, scenarios: List[Dict[str, Any]]) -> Tuple[go.Figure, ChartAnalysis]:
        """Graphique Pareto intelligent avec analyse des zones optimales"""
        
        # Extraction des donn√©es
        durations = [s['duration'] for s in scenarios]
        costs = [s['cost'] for s in scenarios]
        qualities = [s['quality_score'] for s in scenarios]
        success_probs = [s['success_probability'] for s in scenarios]
        names = [s.get('name', f'Sc√©nario {i+1}') for i, s in enumerate(scenarios)]
        
        # Analyse Pareto
        pareto_analysis = self._analyze_pareto_efficiency(scenarios)
        
        # Cr√©ation du graphique 3D interactif
        fig = go.Figure()
        
        # Points principaux avec analyse de dominance
        for i, scenario in enumerate(scenarios):
            is_pareto = pareto_analysis['pareto_optimal'][i]
            domination_count = pareto_analysis['domination_counts'][i]
            
            color = 'gold' if is_pareto else 'lightblue' if domination_count < 2 else 'lightcoral'
            symbol = 'diamond' if is_pareto else 'circle'
            size = 15 + success_probs[i] * 15  # Taille bas√©e sur probabilit√© succ√®s
            
            fig.add_trace(go.Scatter3d(
                x=[durations[i]],
                y=[costs[i]],
                z=[qualities[i]],
                mode='markers+text',
                marker=dict(
                    size=size,
                    color=color,
                    symbol=symbol,
                    line=dict(width=2, color='black')
                ),
                text=[names[i][:10]],
                textposition="top center",
                name=f"{'‚≠ê ' if is_pareto else ''}Sc√©nario {i+1}",
                hovertemplate=(
                    f"<b>{names[i]}</b><br>"
                    "Dur√©e: %{x:.1f} jours<br>"
                    "Co√ªt: $%{y:,.0f}<br>"
                    "Qualit√©: %{z:.1%}<br>"
                    f"Succ√®s: {success_probs[i]:.1%}<br>"
                    f"{'üèÜ Pareto Optimal' if is_pareto else f'Domin√© par {domination_count} solutions'}"
                    "<extra></extra>"
                )
            ))
        
        # Surface Pareto (approximation)
        if len([i for i, is_pareto in enumerate(pareto_analysis['pareto_optimal']) if is_pareto]) > 3:
            pareto_points = [(d, c, q) for d, c, q, is_pareto in 
                           zip(durations, costs, qualities, pareto_analysis['pareto_optimal']) 
                           if is_pareto]
            
            if len(pareto_points) > 3:
                # Tri des points Pareto
                pareto_points.sort()
                
                pareto_durations, pareto_costs, pareto_qualities = zip(*pareto_points)
                
                # Surface approximative
                fig.add_trace(go.Mesh3d(
                    x=pareto_durations,
                    y=pareto_costs, 
                    z=pareto_qualities,
                    opacity=0.3,
                    color='gold',
                    name="Front de Pareto"
                ))
        
        # Zones d'am√©lioration
        best_duration = min(durations)
        best_cost = min(costs)
        best_quality = max(qualities)
        
        fig.add_trace(go.Scatter3d(
            x=[best_duration],
            y=[best_cost],
            z=[best_quality],
            mode='markers',
            marker=dict(size=20, color='lime', symbol='diamond'),
            name="üéØ Id√©al Th√©orique",
            hovertemplate="<b>Solution Id√©ale Th√©orique</b><br>Dur√©e: %{x:.1f}j<br>Co√ªt: $%{y:,.0f}<br>Qualit√©: %{z:.1%}<extra></extra>"
        ))
        
        # Mise en forme
        fig.update_layout(
            title="üéØ Analyse Pareto 3D Intelligente - Zones Optimales Identifi√©es",
            scene=dict(
                xaxis_title="‚è±Ô∏è Dur√©e (jours)",
                yaxis_title="üí∞ Co√ªt ($)",
                zaxis_title="‚≠ê Qualit√© (%)",
                camera=dict(eye=dict(x=1.2, y=1.2, z=0.8))
            ),
            height=700
        )
        
        # G√©n√©ration des insights
        insights = self._generate_pareto_insights(pareto_analysis, scenarios)
        
        analysis = ChartAnalysis(
            chart_type="intelligent_pareto_3d",
            insights=insights,
            statistical_summary=pareto_analysis,
            business_interpretation=self._generate_pareto_interpretation(pareto_analysis),
            recommendations=self._generate_pareto_recommendations(pareto_analysis, scenarios)
        )
        
        return fig, analysis
    
    def create_intelligent_risk_heatmap(self, scenarios: List[Dict[str, Any]], 
                                      risk_factors: List[str]) -> Tuple[go.Figure, ChartAnalysis]:
        """Heatmap de risque intelligente avec clustering automatique"""
        
        # Matrice de risques
        risk_matrix = []
        scenario_names = []
        
        for i, scenario in enumerate(scenarios):
            scenario_risks = []
            for factor in risk_factors:
                # Simulation de scores de risque bas√©s sur les param√®tres du sc√©nario
                base_risk = np.random.uniform(1, 10)  # En r√©alit√©, on utiliserait les vrais calculs
                risk_modifier = self._calculate_risk_modifier(scenario, factor)
                final_risk = min(10, max(1, base_risk * risk_modifier))
                scenario_risks.append(final_risk)
            
            risk_matrix.append(scenario_risks)
            scenario_names.append(scenario.get('name', f'Sc√©nario {i+1}'))
        
        risk_matrix = np.array(risk_matrix)
        
        # Clustering automatique des risques
        if len(scenarios) > 2:
            kmeans = KMeans(n_clusters=min(3, len(scenarios)), random_state=42)
            risk_clusters = kmeans.fit_predict(risk_matrix)
        else:
            risk_clusters = [0] * len(scenarios)
        
        # Cr√©ation de la heatmap avanc√©e
        fig = go.Figure()
        
        # Heatmap principale avec annotations intelligentes
        annotations = []
        for i, scenario_name in enumerate(scenario_names):
            for j, factor in enumerate(risk_factors):
                risk_value = risk_matrix[i][j]
                
                # Couleur et symbole bas√©s sur le niveau de risque
                if risk_value > 7:
                    symbol = "üî¥"
                    text_color = "white"
                elif risk_value > 4:
                    symbol = "üü°"
                    text_color = "black"
                else:
                    symbol = "üü¢"
                    text_color = "black"
                
                annotations.append(
                    dict(
                        x=j, y=i,
                        text=f"{symbol}<br>{risk_value:.1f}",
                        showarrow=False,
                        font=dict(color=text_color, size=10)
                    )
                )
        
        fig.add_trace(go.Heatmap(
            z=risk_matrix,
            x=risk_factors,
            y=scenario_names,
            colorscale='RdYlGn_r',  # Rouge = risque √©lev√©
            reversescale=False,
            hoverongaps=False,
            hovertemplate="<b>%{y}</b><br>Facteur: %{x}<br>Risque: %{z:.1f}/10<extra></extra>",
            colorbar=dict(
                title="Niveau de Risque",
                tickvals=[1, 3, 5, 7, 9],
                ticktext=["Tr√®s Faible", "Faible", "Moyen", "√âlev√©", "Critique"]
            )
        ))
        
        fig.update_layout(
            annotations=annotations,
            title="üö® Analyse de Risque Intelligente - Clustering Automatique",
            xaxis_title="Facteurs de Risque",
            yaxis_title="Sc√©narios",
            height=max(400, len(scenarios) * 50)
        )
        
        # Analyse des patterns de risque
        risk_insights = self._analyze_risk_patterns(risk_matrix, scenario_names, risk_factors)
        
        analysis = ChartAnalysis(
            chart_type="intelligent_risk_heatmap",
            insights=risk_insights,
            statistical_summary={
                "risk_matrix": risk_matrix.tolist(),
                "clusters": risk_clusters.tolist(),
                "avg_risk_per_scenario": np.mean(risk_matrix, axis=1).tolist(),
                "avg_risk_per_factor": np.mean(risk_matrix, axis=0).tolist()
            },
            business_interpretation=self._generate_risk_interpretation(risk_insights),
            recommendations=self._generate_risk_recommendations(risk_insights, scenarios)
        )
        
        return fig, analysis
    
    def create_intelligent_sensitivity_chart(self, base_scenario: Dict[str, Any],
                                           sensitivity_params: List[str]) -> Tuple[go.Figure, ChartAnalysis]:
        """Analyse de sensibilit√© intelligente avec tornado chart"""
        
        # Calcul de sensibilit√© pour chaque param√®tre
        sensitivities = []
        param_ranges = []
        
        for param in sensitivity_params:
            # Variation de ¬±20% du param√®tre
            low_impact, high_impact = self._calculate_parameter_sensitivity(base_scenario, param)
            sensitivity = (high_impact - low_impact) / 2  # Impact moyen
            
            sensitivities.append(abs(sensitivity))
            param_ranges.append((low_impact, high_impact))
        
        # Tri par ordre de sensibilit√©
        sorted_data = sorted(zip(sensitivity_params, sensitivities, param_ranges), 
                           key=lambda x: x[1], reverse=True)
        
        params, sens_values, ranges = zip(*sorted_data)
        
        # Cr√©ation du tornado chart
        fig = go.Figure()
        
        # Barres n√©gatives (impact de -20%)
        fig.add_trace(go.Bar(
            y=params,
            x=[-s/2 for s in sens_values],
            orientation='h',
            name='Impact -20%',
            marker_color='lightcoral',
            hovertemplate="Param√®tre: %{y}<br>Impact: %{x:.1f}<extra></extra>"
        ))
        
        # Barres positives (impact de +20%)
        fig.add_trace(go.Bar(
            y=params,
            x=[s/2 for s in sens_values],
            orientation='h',
            name='Impact +20%',
            marker_color='lightblue',
            hovertemplate="Param√®tre: %{y}<br>Impact: %{x:.1f}<extra></extra>"
        ))
        
        # Ligne de base
        fig.add_vline(x=0, line_dash="dash", line_color="black", line_width=2)
        
        # Annotations avec insights
        annotations = []
        for i, (param, sens, (low, high)) in enumerate(sorted_data):
            # Classification de la sensibilit√©
            if sens > np.mean(sens_values) * 1.5:
                insight = "üî¥ CRITIQUE"
                color = "red"
            elif sens > np.mean(sens_values):
                insight = "üü° √âLEV√â"
                color = "orange"
            else:
                insight = "üü¢ FAIBLE"
                color = "green"
            
            annotations.append(
                dict(
                    x=sens/2 + 0.5,
                    y=i,
                    text=insight,
                    showarrow=False,
                    font=dict(color=color, size=10, family="Arial Black")
                )
            )
        
        fig.update_layout(
            title="üìä Analyse de Sensibilit√© Intelligente - Tornado Chart",
            xaxis_title="Impact sur le R√©sultat Final",
            yaxis_title="Param√®tres de Projet",
            barmode='relative',
            height=max(400, len(params) * 60),
            annotations=annotations
        )
        
        # G√©n√©ration des insights de sensibilit√©
        sensitivity_insights = self._generate_sensitivity_insights(sorted_data)
        
        analysis = ChartAnalysis(
            chart_type="intelligent_sensitivity_tornado",
            insights=sensitivity_insights,
            statistical_summary={
                "sensitivity_ranking": list(params),
                "sensitivity_values": list(sens_values),
                "parameter_ranges": list(ranges),
                "most_sensitive": params[0] if params else None
            },
            business_interpretation=self._generate_sensitivity_interpretation(sorted_data),
            recommendations=self._generate_sensitivity_recommendations(sorted_data)
        )
        
        return fig, analysis
    
    # === M√âTHODES D'ANALYSE PRIV√âES ===
    
    def _calculate_advanced_stats(self, data: np.ndarray) -> Dict[str, Any]:
        """Calcule des statistiques avanc√©es"""
        stats_dict = {
            'mean': float(np.mean(data)),
            'std': float(np.std(data)),
            'var_95': float(np.percentile(data, 95)),
            'var_99': float(np.percentile(data, 99)),
            'coefficient_variation': float(np.std(data) / np.mean(data)) if np.mean(data) != 0 else 0
        }
        
        # Ajout des stats avanc√©es si scipy disponible
        if SCIPY_AVAILABLE:
            try:
                stats_dict['skewness'] = float(stats.skew(data))
                stats_dict['kurtosis'] = float(stats.kurtosis(data))
            except:
                stats_dict['skewness'] = 0.0
                stats_dict['kurtosis'] = 0.0
        else:
            stats_dict['skewness'] = 0.0
            stats_dict['kurtosis'] = 0.0
            
        return stats_dict
    
    def _analyze_pareto_efficiency(self, scenarios: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyse l'efficacit√© Pareto"""
        n = len(scenarios)
        pareto_optimal = [True] * n
        domination_counts = [0] * n
        
        for i in range(n):
            for j in range(n):
                if i != j:
                    # V√©rifier si j domine i
                    if (scenarios[j]['duration'] <= scenarios[i]['duration'] and
                        scenarios[j]['cost'] <= scenarios[i]['cost'] and 
                        scenarios[j]['quality_score'] >= scenarios[i]['quality_score'] and
                        (scenarios[j]['duration'] < scenarios[i]['duration'] or
                         scenarios[j]['cost'] < scenarios[i]['cost'] or
                         scenarios[j]['quality_score'] > scenarios[i]['quality_score'])):
                        
                        pareto_optimal[i] = False
                        domination_counts[i] += 1
        
        return {
            'pareto_optimal': pareto_optimal,
            'domination_counts': domination_counts,
            'pareto_count': sum(pareto_optimal),
            'efficiency_ratio': sum(pareto_optimal) / n if n > 0 else 0
        }
    
    def _calculate_risk_modifier(self, scenario: Dict[str, Any], risk_factor: str) -> float:
        """Calcule le modificateur de risque pour un facteur donn√©"""
        modifiers = {
            'Timeline': scenario.get('duration', 100) / 100,  # Normalis√©
            'Budget': scenario.get('cost', 50000) / 50000,
            'Quality': 2 - scenario.get('quality_score', 0.8),
            'Team': scenario.get('team_size', 5) / 5,
            'Complexity': scenario.get('complexity_factor', 1.0)
        }
        return modifiers.get(risk_factor, 1.0)
    
    def _calculate_parameter_sensitivity(self, base_scenario: Dict[str, Any], 
                                       param: str) -> Tuple[float, float]:
        """Calcule la sensibilit√© d'un param√®tre"""
        base_value = base_scenario.get('duration', 100)  # Simplification
        
        # Simulation de l'impact de ¬±20%
        low_impact = base_value * 0.8
        high_impact = base_value * 1.2
        
        return low_impact, high_impact
    
    def _generate_monte_carlo_insights(self, duration_stats: Dict, cost_stats: Dict, 
                                     quality_stats: Dict, correlation: float) -> List[ChartInsight]:
        """G√©n√®re des insights pour Monte Carlo"""
        insights = []
        
        # Analyse de variance
        if duration_stats['coefficient_variation'] > 0.4:
            insights.append(ChartInsight(
                type="risk",
                title="üö® Variance √âlev√©e D√©tect√©e",
                description=f"Le coefficient de variation de la dur√©e est de {duration_stats['coefficient_variation']:.2f}, indiquant une forte incertitude.",
                confidence=0.9,
                severity="high",
                action_items=[
                    "D√©composer les t√¢ches complexes",
                    "Ajouter des points de contr√¥le fr√©quents",
                    "Pr√©voir une marge de s√©curit√© de 30%+"
                ],
                supporting_data=duration_stats
            ))
        
        # Analyse d'asym√©trie
        if abs(duration_stats['skewness']) > 1.0:
            direction = "positive" if duration_stats['skewness'] > 0 else "n√©gative"
            insights.append(ChartInsight(
                type="pattern",
                title=f"üìà Asym√©trie {direction.title()} Significative",
                description=f"Distribution asym√©trique ({duration_stats['skewness']:.2f}) sugg√®re des risques de d√©passement.",
                confidence=0.8,
                severity="medium",
                action_items=[
                    "Identifier les causes d'asym√©trie",
                    "Pr√©voir des sc√©narios de contingence"
                ],
                supporting_data={'skewness': duration_stats['skewness']}
            ))
        
        # Analyse de corr√©lation
        if abs(correlation) > 0.7:
            insights.append(ChartInsight(
                type="correlation",
                title="üîó Corr√©lation Forte Dur√©e-Co√ªt",
                description=f"Corr√©lation de {correlation:.2f} entre dur√©e et co√ªt - effet de levier important.",
                confidence=0.9,
                severity="medium" if correlation > 0 else "high",
                action_items=[
                    "Exploiter la corr√©lation pour optimisation",
                    "Surveiller l'effet domino dur√©e‚Üíco√ªt"
                ],
                supporting_data={'correlation': correlation}
            ))
        
        return insights
    
    def _generate_pareto_insights(self, pareto_analysis: Dict, scenarios: List[Dict]) -> List[ChartInsight]:
        """G√©n√®re des insights pour l'analyse Pareto"""
        insights = []
        
        efficiency_ratio = pareto_analysis['efficiency_ratio']
        
        if efficiency_ratio < 0.3:
            insights.append(ChartInsight(
                type="opportunity",
                title="üéØ Opportunit√©s d'Optimisation Majeures",
                description=f"Seulement {efficiency_ratio:.1%} des sc√©narios sont Pareto-optimaux. Fort potentiel d'am√©lioration.",
                confidence=0.95,
                severity="high",
                action_items=[
                    "√âliminer les sc√©narios domin√©s",
                    "Explorer l'espace entre solutions Pareto",
                    "Optimiser les param√®tres sous-performants"
                ],
                supporting_data=pareto_analysis
            ))
        
        # Identification du meilleur sc√©nario
        pareto_scenarios = [s for s, is_pareto in zip(scenarios, pareto_analysis['pareto_optimal']) if is_pareto]
        if pareto_scenarios:
            best_scenario = min(pareto_scenarios, key=lambda s: s['duration'] + s['cost']/1000)
            insights.append(ChartInsight(
                type="recommendation",
                title="‚≠ê Sc√©nario Recommand√© Identifi√©",
                description=f"Le sc√©nario optimal combine efficacit√© temporelle et budg√©taire.",
                confidence=0.9,
                severity="low",
                action_items=[
                    "Analyser en d√©tail le sc√©nario optimal",
                    "Valider la faisabilit√© pratique"
                ],
                supporting_data=best_scenario
            ))
        
        return insights
    
    def _analyze_risk_patterns(self, risk_matrix: np.ndarray, scenario_names: List[str], 
                             risk_factors: List[str]) -> List[ChartInsight]:
        """Analyse les patterns de risque"""
        insights = []
        
        # Sc√©nario le plus risqu√©
        avg_risks = np.mean(risk_matrix, axis=1)
        max_risk_idx = np.argmax(avg_risks)
        
        insights.append(ChartInsight(
            type="risk",
            title="‚ö†Ô∏è Sc√©nario Critique Identifi√©",
            description=f"'{scenario_names[max_risk_idx]}' pr√©sente le profil de risque le plus √©lev√© ({avg_risks[max_risk_idx]:.1f}/10).",
            confidence=0.95,
            severity="high",
            action_items=[
                "R√©viser compl√®tement ce sc√©nario",
                "D√©velopper plan de mitigation sp√©cifique",
                "Consid√©rer l'abandon si risques > b√©n√©fices"
            ],
            supporting_data={'scenario_index': max_risk_idx, 'risk_score': avg_risks[max_risk_idx]}
        ))
        
        # Facteur de risque critique
        avg_factor_risks = np.mean(risk_matrix, axis=0)
        max_factor_idx = np.argmax(avg_factor_risks)
        
        insights.append(ChartInsight(
            type="risk",
            title="üéØ Facteur de Risque Syst√©mique",
            description=f"'{risk_factors[max_factor_idx]}' est critique sur tous les sc√©narios ({avg_factor_risks[max_factor_idx]:.1f}/10).",
            confidence=0.9,
            severity="high",
            action_items=[
                f"Plan d'action sp√©cifique pour {risk_factors[max_factor_idx]}",
                "Formation √©quipe sur ce risque",
                "Monitoring continu de ce facteur"
            ],
            supporting_data={'factor_index': max_factor_idx, 'risk_score': avg_factor_risks[max_factor_idx]}
        ))
        
        return insights
    
    def _generate_sensitivity_insights(self, sorted_data: List[Tuple]) -> List[ChartInsight]:
        """G√©n√®re des insights de sensibilit√©"""
        insights = []
        
        if sorted_data:
            most_sensitive_param, sens_value, (low, high) = sorted_data[0]
            
            insights.append(ChartInsight(
                type="critical",
                title="üéØ Param√®tre Critique Identifi√©",
                description=f"'{most_sensitive_param}' est le levier d'impact principal (sensibilit√©: {sens_value:.1f}).",
                confidence=0.95,
                severity="critical",
                action_items=[
                    f"Contr√¥le strict de {most_sensitive_param}",
                    "Monitoring en temps r√©el de ce param√®tre",
                    "Plan de contingence d√©di√©"
                ],
                supporting_data={'parameter': most_sensitive_param, 'sensitivity': sens_value}
            ))
            
        return insights
    
    def _generate_business_interpretation(self, insights: List[ChartInsight]) -> str:
        """G√©n√®re l'interpr√©tation business"""
        critical_count = len([i for i in insights if i.severity in ['high', 'critical']])
        
        if critical_count > 2:
            return "üö® **ATTENTION**: Plusieurs risques critiques d√©tect√©s. R√©vision strat√©gique recommand√©e avant ex√©cution."
        elif critical_count > 0:
            return "‚ö†Ô∏è **VIGILANCE**: Quelques points d'attention identifi√©s. Surveillance renforc√©e n√©cessaire."
        else:
            return "‚úÖ **CONFIANT**: Profil de risque acceptable. Projet pr√™t pour ex√©cution avec monitoring standard."
    
    def _generate_recommendations(self, insights: List[ChartInsight]) -> List[str]:
        """G√©n√®re des recommandations consolid√©es"""
        all_actions = []
        for insight in insights:
            all_actions.extend(insight.action_items)
        
        # D√©duplication et priorisation
        unique_actions = list(set(all_actions))
        return unique_actions[:5]  # Top 5 recommandations
    
    def _generate_pareto_interpretation(self, pareto_analysis: Dict) -> str:
        """Interpr√©tation business de l'analyse Pareto"""
        ratio = pareto_analysis['efficiency_ratio']
        
        if ratio > 0.7:
            return "üéØ **EXCELLENT**: Haute efficacit√© des sc√©narios - espace d'optimisation limit√© mais solutions robustes."
        elif ratio > 0.4:
            return "‚öñÔ∏è **BALANC√â**: Efficacit√© mod√©r√©e - opportunit√©s d'optimisation disponibles sans risque majeur."
        else:
            return "üöÄ **FORT POTENTIEL**: Nombreuses opportunit√©s d'am√©lioration - optimisation recommand√©e avant d√©cision finale."
    
    def _generate_pareto_recommendations(self, pareto_analysis: Dict, scenarios: List[Dict]) -> List[str]:
        """Recommandations bas√©es sur l'analyse Pareto"""
        recommendations = []
        
        if pareto_analysis['efficiency_ratio'] < 0.5:
            recommendations.append("üéØ √âliminer les sc√©narios non Pareto-optimaux")
            recommendations.append("üîç Explorer l'espace entre solutions optimales")
        
        recommendations.append("üìä Valider la faisabilit√© des sc√©narios Pareto")
        recommendations.append("üîÑ It√©rer sur les param√®tres des solutions domin√©es")
        
        return recommendations
    
    def _generate_risk_interpretation(self, risk_insights: List[ChartInsight]) -> str:
        """Interpr√©tation des risques"""
        high_risk_count = len([i for i in risk_insights if i.severity == 'high'])
        
        if high_risk_count > 1:
            return "üö® **RISQUE √âLEV√â**: Plusieurs facteurs critiques identifi√©s - mitigation urgente requise."
        elif high_risk_count == 1:
            return "‚ö†Ô∏è **VIGILANCE**: Un risque critique identifi√© - plan d'action cibl√© n√©cessaire."
        else:
            return "üõ°Ô∏è **MA√éTRIS√â**: Profil de risque acceptable - monitoring standard suffisant."
    
    def _generate_risk_recommendations(self, risk_insights: List[ChartInsight], scenarios: List[Dict]) -> List[str]:
        """Recommandations de gestion des risques"""
        recommendations = []
        
        for insight in risk_insights:
            if insight.severity in ['high', 'critical']:
                recommendations.extend(insight.action_items[:2])  # Top 2 par insight critique
        
        return list(set(recommendations))[:5]
    
    def _generate_sensitivity_interpretation(self, sorted_data: List[Tuple]) -> str:
        """Interpr√©tation de l'analyse de sensibilit√©"""
        if not sorted_data:
            return "üìä Analyse de sensibilit√© non disponible."
        
        top_param = sorted_data[0][0]
        return f"üéØ **FOCUS**: '{top_param}' est le levier principal de performance - concentration des efforts recommand√©e sur ce param√®tre."
    
    def _generate_sensitivity_recommendations(self, sorted_data: List[Tuple]) -> List[str]:
        """Recommandations bas√©es sur la sensibilit√©"""
        if not sorted_data:
            return ["üîç Collecter plus de donn√©es pour analyse de sensibilit√©"]
        
        top_3_params = [data[0] for data in sorted_data[:3]]
        
        recommendations = [
            f"üéØ Contr√¥le prioritaire: {top_3_params[0]}",
            f"üìä Monitoring renforc√©: {', '.join(top_3_params[1:3])}",
            "üîÑ Calibrage r√©gulier des param√®tres sensibles",
            "üìà Dashboard temps r√©el pour param√®tres critiques"
        ]
        
        return recommendations

def render_chart_intelligence_panel(analysis: ChartAnalysis):
    """Panneau d'intelligence graphique avec insights"""
    
    st.markdown("---")
    st.markdown("## üß† **Intelligence Graphique Automatique**")
    
    # R√©sum√© ex√©cutif
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.markdown("### üìã Interpr√©tation Business")
        st.info(analysis.business_interpretation)
    
    with col2:
        # M√©triques cl√©s
        st.markdown("### üìä M√©triques")
        critical_insights = len([i for i in analysis.insights if i.severity in ['high', 'critical']])
        st.metric("Insights Critiques", critical_insights)
        st.metric("Confiance Moyenne", f"{np.mean([i.confidence for i in analysis.insights]):.1%}")
    
    # Insights d√©taill√©s
    st.markdown("### üîç Insights Automatiques")
    
    for insight in analysis.insights:
        # Couleur bas√©e sur la s√©v√©rit√©
        if insight.severity == 'critical':
            color = "üî¥"
            container = st.error
        elif insight.severity == 'high':
            color = "üü†"
            container = st.warning
        elif insight.severity == 'medium':
            color = "üü°"
            container = st.info
        else:
            color = "üü¢"
            container = st.success
        
        with container(f"{color} **{insight.title}**"):
            st.markdown(insight.description)
            
            if insight.action_items:
                st.markdown("**Actions recommand√©es:**")
                for action in insight.action_items:
                    st.markdown(f"‚Ä¢ {action}")
            
            with st.expander("Donn√©es d√©taill√©es"):
                st.json(insight.supporting_data)
    
    # Recommandations consolid√©es
    if analysis.recommendations:
        st.markdown("### üéØ Plan d'Action Recommand√©")
        
        for i, rec in enumerate(analysis.recommendations, 1):
            st.markdown(f"**{i}.** {rec}")
    
    # Export des insights
    import time
    unique_key = f"export_chart_analysis_{int(time.time() * 1000)}"
    if st.button("üìÑ Exporter Analyse Compl√®te", key=unique_key):
        export_data = {
            'timestamp': datetime.now().isoformat(),
            'chart_type': analysis.chart_type,
            'business_interpretation': analysis.business_interpretation,
            'insights': [
                {
                    'type': i.type,
                    'title': i.title,
                    'description': i.description,
                    'severity': i.severity,
                    'confidence': i.confidence,
                    'action_items': i.action_items
                }
                for i in analysis.insights
            ],
            'recommendations': analysis.recommendations,
            'statistical_summary': analysis.statistical_summary
        }
        
        st.download_button(
            label="‚¨áÔ∏è T√©l√©charger Rapport d'Analyse",
            data=json.dumps(export_data, indent=2, default=str),
            file_name=f"chart_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json"
        )

# Export des classes principales
__all__ = [
    'IntelligentChartAnalyzer',
    'ChartInsight', 
    'ChartAnalysis',
    'render_chart_intelligence_panel'
]